from sentence_transformers import SentenceTransformer
import numpy as np
import pandas as pd
import pyodbc
import json
from sklearn.metrics.pairwise import cosine_similarity
from underthesea import ner , pos_tag 
from itertools import groupby
import re 
import os
from dotenv import load_dotenv

# Load environment variables
load_dotenv()

# C·∫•u h√¨nh database t·ª´ environment variables
DATABASE_CONFIG = {
    'driver': os.getenv('DB_DRIVER'),
    'server': os.getenv('DB_SERVER'),
    'database': os.getenv('DB_DATABASE'),
    'trusted_connection': os.getenv('DB_TRUSTED_CONNECTION')
}

# Load model t·ª´ environment variable
model_name = os.getenv('EMBEDDING_MODEL')
embedding_model = SentenceTransformer(model_name)


# Cache cho documents t·ª´ database
_documents_cache = None
_embeddings_cache = None
_doc_table_map_cache = None

def get_database_connection():
    """T·∫°o k·∫øt n·ªëi ƒë·∫øn SQL Server"""
    try:
        if 'username' in DATABASE_CONFIG and 'password' in DATABASE_CONFIG:
            conn_str = f"""
            DRIVER={{{DATABASE_CONFIG['driver']}}};
            SERVER={DATABASE_CONFIG['server']};
            DATABASE={DATABASE_CONFIG['database']};
            UID={DATABASE_CONFIG['username']};
            PWD={DATABASE_CONFIG['password']};
            """
        else:
            conn_str = f"""
            DRIVER={{{DATABASE_CONFIG['driver']}}};
            SERVER={DATABASE_CONFIG['server']};
            DATABASE={DATABASE_CONFIG['database']};
            Trusted_Connection={DATABASE_CONFIG['trusted_connection']};
            """
        
        conn = pyodbc.connect(conn_str)
        return conn
    except Exception as e:
        print(f"‚ùå L·ªói k·∫øt n·ªëi database: {e}")
        return None

def load_documents_from_db():
    """Load documents v√† embeddings t·ª´ database v·ªõi cache"""
    global _documents_cache, _embeddings_cache, _doc_table_map_cache
    
    # N·∫øu ƒë√£ c√≥ cache, tr·∫£ v·ªÅ lu√¥n
    if _documents_cache is not None:
        return _documents_cache, _embeddings_cache, _doc_table_map_cache
    
    conn = get_database_connection()
    if not conn:
        return {}, [], {}
    
    try:
        query = "SELECT id, content, embedding FROM document_embeddings ORDER BY id"
        df = pd.read_sql(query, conn)
        
        doc_map = {}
        embeddings = []
        doc_table_map = {}
        
        for idx, row in df.iterrows():
            doc_id = row['id']
            content = row['content']
            
            # Parse embedding t·ª´ JSON
            embedding = np.array(json.loads(row['embedding']))
            embeddings.append(embedding)
            
            # Parse content v√† metadata
            if content.startswith('METADATA:'):
                try:
                    parts = content.split('\n\nCONTENT:\n', 1)
                    if len(parts) == 2:
                        metadata_str = parts[0].replace('METADATA: ', '')
                        metadata = json.loads(metadata_str)
                        
                        actual_content = parts[1]
                        table_name = metadata.get('table_name', 'unknown')
                        original_doc_id = metadata.get('doc_id', doc_id)
                        
                        doc_map[original_doc_id] = actual_content
                        doc_table_map[original_doc_id] = table_name
                    else:
                        doc_map[doc_id] = content
                        doc_table_map[doc_id] = 'unknown'
                except:
                    doc_map[doc_id] = content
                    doc_table_map[doc_id] = 'unknown'
            else:
                doc_map[doc_id] = content
                doc_table_map[doc_id] = 'unknown'
        
        # Cache k·∫øt qu·∫£
        _documents_cache = doc_map
        _embeddings_cache = np.array(embeddings) if embeddings else np.array([])
        _doc_table_map_cache = doc_table_map
        
        conn.close()
        print(f"‚úÖ ƒê√£ load {len(doc_map)} documents t·ª´ database")
        return doc_map, _embeddings_cache, doc_table_map
        
    except Exception as e:
        print(f"‚ùå L·ªói load documents t·ª´ database: {e}")
        conn.close()
        return {}, [], {}

# Load documents khi kh·ªüi t·∫°o module
doc_map, embeddings_matrix, doc_table_map = load_documents_from_db()

def refresh_cache():
    """L√†m m·ªõi cache t·ª´ database"""
    global _documents_cache, _embeddings_cache, _doc_table_map_cache, doc_map, embeddings_matrix, doc_table_map
    
    _documents_cache = None
    _embeddings_cache = None
    _doc_table_map_cache = None
    
    doc_map, embeddings_matrix, doc_table_map = load_documents_from_db()
    print("üîÑ ƒê√£ l√†m m·ªõi cache t·ª´ database")

def get_database_stats():
    """L·∫•y th·ªëng k√™ v·ªÅ database"""
    conn = get_database_connection()
    if not conn:
        return "‚ùå Kh√¥ng th·ªÉ k·∫øt n·ªëi database"
    
    try:
        cursor = conn.cursor()
        cursor.execute("SELECT COUNT(*) FROM document_embeddings")
        count = cursor.fetchone()[0]
        
        cursor.execute("SELECT COUNT(DISTINCT JSON_VALUE(content, '$.table_name')) FROM document_embeddings WHERE content LIKE 'METADATA:%'")
        table_count = cursor.fetchone()[0]
        
        conn.close()
        return f"üìä Database c√≥ {count} documents t·ª´ {table_count} b·∫£ng kh√°c nhau"
    
    except Exception as e:
        conn.close()
        return f"‚ùå L·ªói l·∫•y th·ªëng k√™ database: {e}"






def search_exact_customer(customer_name):

    # Chu·∫©n h√≥a t√™n t√¨m ki·∫øm
    customer_name = customer_name.strip().lower()
    
    found_customers = []
    
    # T√¨m trong t·∫•t c·∫£ documents
    for doc_id, content in doc_map.items():
        # Ch·ªâ t√¨m trong documents c·ªßa b·∫£ng customers
        table_name = doc_table_map.get(doc_id, '')
        if table_name == 'customers':
            content_lower = content.lower()
            
            # 1. "kh√°ch h√†ng: t√™n"
            if f"kh√°ch h√†ng: {customer_name}" in content_lower:
                found_customers.append(content)
            # 2. "t√™n kh√°ch h√†ng: t√™n"  
            elif f"t√™n kh√°ch h√†ng: {customer_name}" in content_lower:
                found_customers.append(content)
            # 3. "h·ªç t√™n: t√™n"
            elif f"h·ªç t√™n: {customer_name}" in content_lower:
                found_customers.append(content)
            # 4. "name: t√™n"
            elif f"name: {customer_name}" in content_lower:
                found_customers.append(content)
            # 5. "t√™n: t√™n"
            elif f"t√™n: {customer_name}" in content_lower:
                found_customers.append(content)
            # 6. "fullname: t√™n"
            elif f"fullname: {customer_name}" in content_lower:
                found_customers.append(content)
            # 7. T√¨m t√™n trong b·∫•t k·ª≥ ƒë√¢u trong content (loose matching)
            elif customer_name in content_lower:
                found_customers.append(content)
    
    if found_customers:
        if len(found_customers) == 1:
            return found_customers[0]
        else:
            # Tr·∫£ v·ªÅ t·∫•t c·∫£ kh√°ch h√†ng c√πng t√™n
            result = f"T√¨m th·∫•y {len(found_customers)} kh√°ch h√†ng c√≥ t√™n '{customer_name}':\n\n"
            for i, customer in enumerate(found_customers, 1):
                result += f"=== KH√ÅCH H√ÄNG {i} ===\n{customer}\n\n"
            return result.strip()
    
    return None

def search_customer_by_phone(phone):

    phone_clean = re.sub(r'[\s\-\(\)\+]', '', phone.strip())
    
    phone_variants = [phone_clean]
    
    # N·∫øu b·∫Øt ƒë·∫ßu b·∫±ng 84, th√™m bi·∫øn th·ªÉ v·ªõi 0
    if phone_clean.startswith('84') and len(phone_clean) == 11:
        phone_variants.append('0' + phone_clean[2:])
    
    # N·∫øu b·∫Øt ƒë·∫ßu b·∫±ng 0, th√™m bi·∫øn th·ªÉ v·ªõi 84
    if phone_clean.startswith('0') and len(phone_clean) == 10:
        phone_variants.append('84' + phone_clean[1:])
    
    # T√¨m trong t·∫•t c·∫£ documents
    for doc_id, content in doc_map.items():
        # Ch·ªâ t√¨m trong documents c·ªßa b·∫£ng customers
        table_name = doc_table_map.get(doc_id, '')
        if table_name == 'customers':
            # T√¨m s·ªë ƒëi·ªán tho·∫°i trong n·ªôi dung (lo·∫°i b·ªè k√Ω t·ª± ƒë·∫∑c bi·ªát khi so s√°nh)
            content_clean = re.sub(r'[\s\-\(\)\+]', '', content)
            
            # Ki·ªÉm tra t·∫•t c·∫£ c√°c bi·∫øn th·ªÉ
            for variant in phone_variants:
                if variant in content_clean:
                    return content
    
    return None

def search_customer_by_email(email):

    email = email.strip().lower()
    
    for doc_id, content in doc_map.items():
        # Ch·ªâ t√¨m trong documents c·ªßa b·∫£ng customers
        table_name = doc_table_map.get(doc_id, '')
        if table_name == 'customers':
            # Ki·ªÉm tra n·∫øu email c√≥ trong n·ªôi dung
            if email in content.lower():
                return content
    
    return None




def search_customer_comprehensive(query):
    """
    T√¨m ki·∫øm kh√°ch h√†ng to√†n di·ªán theo t√™n, s·ªë ƒëi·ªán tho·∫°i ho·∫∑c email
    
    Args:
        query (str): Truy v·∫•n t√¨m ki·∫øm (c√≥ th·ªÉ ch·ª©a t√™n, s·ªë ƒëi·ªán tho·∫°i ho·∫∑c email)
        
    Returns:
        dict: K·∫øt qu·∫£ t√¨m ki·∫øm v·ªõi th√¥ng tin v·ªÅ ph∆∞∆°ng th·ª©c t√¨m th·∫•y
    """
    # Th·ª≠ c√°c ph∆∞∆°ng th·ª©c t√¨m ki·∫øm theo th·ª© t·ª± ∆∞u ti√™n
    
    # 1. T√¨m ki·∫øm theo email
    email_pattern = r'\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Z|a-z]{2,}\b'
    email_matches = re.findall(email_pattern, query)
    if email_matches:
        for email in email_matches:
            result = search_customer_by_email(email)
            if result:
                return {
                    'found': True,
                    'method': 'email',
                    'search_term': email,
                    'content': result
                }
    
    # 2. T√¨m ki·∫øm theo s·ªë ƒëi·ªán tho·∫°i
    phone_patterns = [
        r'\b0\d{9}\b',  # S·ªë ƒëi·ªán tho·∫°i Vi·ªát Nam b·∫Øt ƒë·∫ßu b·∫±ng 0 (10 s·ªë)
        r'\b\+84\d{9}\b',  # S·ªë ƒëi·ªán tho·∫°i qu·ªëc t·∫ø v·ªõi +84
        r'\b84\d{9}\b',  # S·ªë ƒëi·ªán tho·∫°i qu·ªëc t·∫ø kh√¥ng c√≥ d·∫•u +
        r'\b0\d{2}[\s\-]?\d{3}[\s\-]?\d{4}\b',  # S·ªë c√≥ d·∫•u ph√¢n c√°ch d·∫°ng 0xx-xxx-xxxx
        r'\b0\d{3}[\s\-]?\d{3}[\s\-]?\d{3}\b',   # S·ªë c√≥ d·∫•u ph√¢n c√°ch d·∫°ng 0xxx-xxx-xxx
        r'\b\d{10}\b',  # B·∫•t k·ª≥ d√£y 10 s·ªë n√†o (c√≥ th·ªÉ l√† s·ªë ƒëi·ªán tho·∫°i)
        r'\b\d{11}\b'   # B·∫•t k·ª≥ d√£y 11 s·ªë n√†o (c√≥ th·ªÉ l√† s·ªë ƒëi·ªán tho·∫°i qu·ªëc t·∫ø)
    ]
    
    for pattern in phone_patterns:
        phone_matches = re.findall(pattern, query)
        if phone_matches:
            for phone in phone_matches:
                result = search_customer_by_phone(phone)
                if result:
                    return {
                        'found': True,
                        'method': 'phone',
                        'search_term': phone,
                        'content': result
                    }
    
    # 3. T√¨m ki·∫øm theo t√™n
    name_patterns = [
        r'th√¥ng tin kh√°ch h√†ng[:\s]+([^,\n]+)',
        r'kh√°ch h√†ng[:\s]+([^,\n]+)',
        r't√¨m.*?([A-Zƒê√Ä-·ª∏][a-z√†-·ªπ]+(?:\s+[A-Zƒê√Ä-·ª∏][a-z√†-·ªπ]+)*)',
        r'([A-Zƒê√Ä-·ª∏][a-z√†-·ªπ]+(?:\s+[A-Zƒê√Ä-·ª∏][a-z√†-·ªπ]+)+)'
    ]
    
    for pattern in name_patterns:
        matches = re.findall(pattern, query)
        if matches:
            customer_name = matches[0].strip()
            result = search_exact_customer(customer_name)
            if result:
                return {
                    'found': True,
                    'method': 'name',
                    'search_term': customer_name,
                    'content': result
                }
    
    return {
        'found': False,
        'method': None,
        'search_term': None,
        'content': None
    }

def retrieve_top_k(query, k):
    """
    T√¨m ki·∫øm th√¥ng tin v·ªõi quy tr√¨nh 2 b∆∞·ªõc:
    1. T√¨m ki·∫øm keyword tr∆∞·ªõc (exact search cho kh√°ch h√†ng)
    2. D√πng k·∫øt qu·∫£ keyword ƒë·ªÉ l√†m semantic search, ho·∫∑c semantic search tr·ª±c ti·∫øp n·∫øu kh√¥ng c√≥ keyword
    """
    global embeddings_matrix, doc_map
    
    if len(embeddings_matrix) == 0:
        return "‚ùå Kh√¥ng c√≥ d·ªØ li·ªáu embedding trong database. Vui l√≤ng ch·∫°y LoadData.py tr∆∞·ªõc."
    
    #Th·ª≠ t√¨m ki·∫øm ch√≠nh x√°c kh√°ch h√†ng tr∆∞·ªõc (keyword search)
    search_result = search_customer_comprehensive(query)
    
    # N·∫øu t√¨m th·∫•y k·∫øt qu·∫£ keyword, d√πng n√≥ l√†m c∆° s·ªü cho semantic search
    if search_result['found']:
        method_name = {
            'email': 'email',
            'phone': 's·ªë ƒëi·ªán tho·∫°i', 
            'name': 't√™n'
        }
        
        keyword_content = search_result['content']
        
        #  D√πng k·∫øt qu·∫£ keyword ƒë·ªÉ l√†m semantic search
        # T·∫°o embedding cho keyword result v√† query k·∫øt h·ª£p
        combined_query = f"{query} {keyword_content}"
        query_embedding = embedding_model.encode([combined_query], convert_to_numpy=True)
        
        # T√≠nh cosine similarity v·ªõi t·∫•t c·∫£ embeddings
        similarities = cosine_similarity(query_embedding, embeddings_matrix)[0]
        
        # L·∫•y top k indices c√≥ similarity cao nh·∫•t
        top_indices = np.argsort(similarities)[::-1][:k]
        
        # L·∫•y vƒÉn b·∫£n t∆∞∆°ng ·ª©ng
        results = []
        
        # Lu√¥n bao g·ªìm k·∫øt qu·∫£ keyword ƒë·∫ßu ti√™n
        # results.append(f"üéØ T√¨m th·∫•y kh√°ch h√†ng theo {method_name[search_result['method']]}: {search_result['search_term']}\n{keyword_content}")
        
        # Th√™m c√°c k·∫øt qu·∫£ semantic li√™n quan (lo·∫°i b·ªè tr√πng l·∫∑p v·ªõi keyword result)
        for idx in top_indices[:k-1]:  # L·∫•y k-1 v√¨ ƒë√£ c√≥ 1 k·∫øt qu·∫£ keyword
            if idx in doc_map and similarities[idx] > 0.1:
                content = doc_map[idx]
                # Ki·ªÉm tra kh√¥ng tr√πng v·ªõi k·∫øt qu·∫£ keyword
                if content != keyword_content:
                    similarity_score = similarities[idx]
                    results.append(f"üìä ƒê·ªô t∆∞∆°ng ƒë·ªìng: {similarity_score:.3f}\n{content}")
        
        return 'üîç K·∫øt qu·∫£ t√¨m ki·∫øm k·∫øt h·ª£p (Keyword + Semantic):\n\n' + '\n\n---\n\n'.join(results)
    
    # N·∫øu kh√¥ng t√¨m th·∫•y keyword, d√πng semantic search tr·ª±c ti·∫øp
    # T·∫°o embedding cho query
    query_embedding = embedding_model.encode([query], convert_to_numpy=True)
    
    # T√≠nh cosine similarity v·ªõi t·∫•t c·∫£ embeddings
    similarities = cosine_similarity(query_embedding, embeddings_matrix)[0]
    
    # L·∫•y top k indices c√≥ similarity cao nh·∫•t
    top_indices = np.argsort(similarities)[::-1][:k]
    
    # Ki·ªÉm tra n·∫øu kh√¥ng t√¨m th·∫•y k·∫øt qu·∫£ n√†o c√≥ similarity ƒë·ªß cao
    if len(top_indices) == 0 or similarities[top_indices[0]] < 0.1:
        return "‚ùå Kh√¥ng t√¨m th·∫•y th√¥ng tin ph√π h·ª£p. Vui l√≤ng ki·ªÉm tra l·∫°i t√™n kh√°ch h√†ng, s·ªë ƒëi·ªán tho·∫°i, email ho·∫∑c th·ª≠ v·ªõi t·ª´ kh√≥a kh√°c."
    
    # L·∫•y vƒÉn b·∫£n t∆∞∆°ng ·ª©ng
    results = []
    for idx in top_indices:
        if idx in doc_map and similarities[idx] > 0.1:  # Ch·ªâ l·∫•y nh·ªØng k·∫øt qu·∫£ c√≥ similarity > 0.1
            similarity_score = similarities[idx]
            content = doc_map[idx]
            results.append(f"üìä ƒê·ªô t∆∞∆°ng ƒë·ªìng: {similarity_score:.3f}\n{content}")
    
    if not results:
        return "‚ùå Kh√¥ng t√¨m th·∫•y th√¥ng tin ph√π h·ª£p. Vui l√≤ng ki·ªÉm tra l·∫°i t√™n kh√°ch h√†ng, s·ªë ƒëi·ªán tho·∫°i, email ho·∫∑c th·ª≠ v·ªõi t·ª´ kh√≥a kh√°c."
    
    return 'üîç K·∫øt qu·∫£ t√¨m ki·∫øm ng·ªØ nghƒ©a t·ª´ database:\n\n' + '\n\n---\n\n'.join(results)







