model_name_or_path: TheBloke/Llama-2-7B-Chat-GGUF
dataset_dir: ../data
output_dir: ../ollama/adapter_model

train_args:
  num_train_epochs: 3
  per_device_train_batch_size: 4
  gradient_accumulation_steps: 4
  learning_rate: 2e-4
  max_grad_norm: 0.3
  logging_steps: 10
  save_strategy: "epoch"
  evaluation_strategy: "epoch"
  
lora_args:
  r: 8
  lora_alpha: 32
  target_modules: ["q_proj", "k_proj", "v_proj", "o_proj"]
  bias: "none"
  task_type: "CAUSAL_LM" 