# ğŸ¤– HÆ°á»›ng dáº«n Setup Model GPT-OSS 20B vá»›i Ollama

## ğŸ“‹ YÃªu cáº§u há»‡ thá»‘ng

- **RAM**: Tá»‘i thiá»ƒu 32GB (khuyáº¿n nghá»‹ 64GB+)
- **GPU**: NVIDIA GPU vá»›i tá»‘i thiá»ƒu 24GB VRAM (khuyáº¿n nghá»‹ RTX 4090/A100)
- **á»” cá»©ng**: Tá»‘i thiá»ƒu 50GB trá»‘ng
- **Docker**: Docker Desktop Ä‘Ã£ cÃ i Ä‘áº·t vÃ  cháº¡y
- **Python**: Python 3.9+ vá»›i pip

---

## ğŸš€ BÆ°á»›c 1: CÃ i Ä‘áº·t Ollama

### 1.1. Táº£i vÃ  cÃ i Ollama

**Windows:**
```powershell
# Táº£i tá»« trang chá»§: https://ollama.ai/download
# Hoáº·c dÃ¹ng winget
winget install Ollama.Ollama
```

**macOS:**
```bash
curl -fsSL https://ollama.ai/install.sh | sh
```

**Linux:**
```bash
curl -fsSL https://ollama.ai/install.sh | sh
```

### 1.2. Khá»Ÿi Ä‘á»™ng Ollama service

```bash
# Khá»Ÿi Ä‘á»™ng Ollama
ollama serve

# Hoáº·c cháº¡y nhÆ° service (Linux/macOS)
sudo systemctl start ollama
```

---

## ğŸ“¦ BÆ°á»›c 2: Táº£i Model GPT-OSS 20B

### 2.1. Táº£i model tá»« Ollama Hub

```bash
# Táº£i model GPT-OSS 20B
ollama pull gpt-oss:20b

# Kiá»ƒm tra model Ä‘Ã£ táº£i
ollama list
```

### 2.2. Hoáº·c táº£i tá»« Hugging Face (náº¿u cáº§n)

```bash
# Náº¿u model chÆ°a cÃ³ trÃªn Ollama Hub
git clone https://huggingface.co/microsoft/gpt-oss-20b
cd gpt-oss-20b

# Convert sang format Ollama
ollama create gpt-oss:20b -f Modelfile
```

---

## ğŸ”§ BÆ°á»›c 3: Deploy Model trÃªn Ollama

### 3.1. Táº¡o Modelfile tÃ¹y chá»‰nh (tÃ¹y chá»n)

Táº¡o file `Modelfile` Ä‘á»ƒ cáº¥u hÃ¬nh model:

```dockerfile
FROM gpt-oss:20b

# Cáº¥u hÃ¬nh tham sá»‘
PARAMETER temperature 0.7
PARAMETER top_p 0.9
PARAMETER num_ctx 4096
PARAMETER num_predict 512

# System prompt cho spa
SYSTEM """
Báº¡n lÃ  trá»£ lÃ½ áº£o chuyÃªn nghiá»‡p cá»§a spa. Nhiá»‡m vá»¥ cá»§a báº¡n lÃ :
1. Tráº£ lá»i cÃ¡c cÃ¢u há»i vá» dá»‹ch vá»¥, giÃ¡ cáº£ vÃ  lá»‹ch háº¹n
2. TÆ° váº¥n dá»‹ch vá»¥ phÃ¹ há»£p cho khÃ¡ch hÃ ng  
3. Há»— trá»£ Ä‘áº·t lá»‹ch vÃ  thÃ´ng tin liÃªn há»‡
4. Tráº£ lá»i báº±ng tiáº¿ng Viá»‡t, lá»‹ch sá»± vÃ  chuyÃªn nghiá»‡p
"""
```

### 3.2. Táº¡o model custom

```bash
# Táº¡o model vá»›i Modelfile tÃ¹y chá»‰nh
ollama create spa-gpt-oss:20b -f Modelfile

# Hoáº·c sá»­ dá»¥ng model gá»‘c
ollama pull gpt-oss:20b
```

### 3.3. Test model

```bash
# Test model hoáº¡t Ä‘á»™ng
ollama run gpt-oss:20b "Xin chÃ o, báº¡n lÃ  ai?"

# Test vá»›i cÃ¢u há»i spa
ollama run gpt-oss:20b "Spa cÃ³ nhá»¯ng dá»‹ch vá»¥ gÃ¬?"
```

---

## âš™ï¸ BÆ°á»›c 4: Cáº¥u hÃ¬nh Environment Variables

### 4.1. Táº¡o/cáº­p nháº­t file `.env`

Trong thÆ° má»¥c project, táº¡o file `.env`:

```env
# Ollama Configuration
OLLAMA_URL=http://localhost:11434
OLLAMA_MODEL=gpt-oss:20b

# Database Configuration
DB_DRIVER={SQL Server}
DB_SERVER=localhost
DB_DATABASE=SpaDB
DB_TRUSTED_CONNECTION=yes

# Embedding Model
EMBEDDING_MODEL=VoVanPhuc/sup-SimCSE-VietNamese-phobert-base
```

### 4.2. CÃ i Ä‘áº·t dependencies

```bash
# CÃ i thÆ° viá»‡n Python cáº§n thiáº¿t
pip install requests python-dotenv flask

# Hoáº·c tá»« requirements.txt
pip install -r requirements.txt
```

---

## ğŸ”Œ BÆ°á»›c 5: TÃ­ch há»£p vÃ o Code

### 5.1. Cáº¥u trÃºc project

```
spa-bot-trainer-pipeline/
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ modules/
â”‚   â”‚   â”œâ”€â”€ query.py          # ÄÃ£ cÃ³ sáºµn
â”‚   â”‚   â””â”€â”€ retrived_rag.py   # ÄÃ£ cÃ³ sáºµn
â”‚   â””â”€â”€ proxy.py              # ÄÃ£ cÃ³ sáºµn
â”œâ”€â”€ .env                      # Táº¡o má»›i
â””â”€â”€ requirements.txt          # Táº¡o má»›i
```

### 5.2. Code Ä‘Ã£ Ä‘Æ°á»£c tÃ­ch há»£p

Model Ä‘Ã£ Ä‘Æ°á»£c tÃ­ch há»£p vÃ o:
- **`modules/query.py`**: Xá»­ lÃ½ RAG vÃ  gá»i Ollama
- **`proxy.py`**: API endpoint cho chat
- **`.env`**: Cáº¥u hÃ¬nh model vÃ  URL

### 5.3. Kiá»ƒm tra káº¿t ná»‘i

```python
# Test script Ä‘á»ƒ kiá»ƒm tra káº¿t ná»‘i
import requests

def test_ollama():
    try:
        response = requests.post(
            "http://localhost:11434/api/generate",
            json={
                "model": "gpt-oss:20b",
                "prompt": "Xin chÃ o",
                "stream": False
            },
            timeout=30
        )
        print("âœ… Ollama hoáº¡t Ä‘á»™ng:", response.json()['response'])
    except Exception as e:
        print("âŒ Lá»—i:", e)

test_ollama()
```

---

## ğŸš€ BÆ°á»›c 6: Cháº¡y Há»‡ thá»‘ng

### 6.1. Khá»Ÿi Ä‘á»™ng cÃ¡c service

```bash
# Terminal 1: Khá»Ÿi Ä‘á»™ng Ollama
ollama serve

# Terminal 2: Khá»Ÿi Ä‘á»™ng Flask app
cd scripts
python proxy.py
```

### 6.2. Test há»‡ thá»‘ng

```bash
# Test API endpoint
curl -X POST http://localhost:5000/bot-msg \
  -H "Content-Type: application/x-www-form-urlencoded" \
  -d "msg=ThÃ´ng tin khÃ¡ch hÃ ng Nguyá»…n Thá»‹ Hoa"
```

### 6.3. Truy cáº­p web interface

Má»Ÿ trÃ¬nh duyá»‡t: `http://localhost:5000`

---

## ğŸ”§ Troubleshooting

### Lá»—i thÆ°á»ng gáº·p:

#### 1. **Model khÃ´ng táº£i Ä‘Æ°á»£c**
```bash
# Kiá»ƒm tra dung lÆ°á»£ng
df -h

# XÃ³a model cÅ© náº¿u cáº§n
ollama rm old-model
```

#### 2. **Timeout khi generate**
- TÄƒng timeout trong code (Ä‘Ã£ set 300s)
- Kiá»ƒm tra GPU memory: `nvidia-smi`
- Giáº£m context length trong Modelfile

#### 3. **Out of Memory**
```bash
# Kiá»ƒm tra RAM
free -h

# Restart Ollama
sudo systemctl restart ollama
```

#### 4. **Káº¿t ná»‘i tháº¥t báº¡i**
```bash
# Kiá»ƒm tra Ollama Ä‘ang cháº¡y
ps aux | grep ollama

# Kiá»ƒm tra port
netstat -an | grep 11434
```

---

## âš¡ Tips Tá»‘i Æ°u

### 1. **TÄƒng tá»‘c Ä‘á»™**
- Sá»­ dá»¥ng GPU cÃ³ VRAM cao
- Set `num_gpu` trong Modelfile
- Giáº£m `num_ctx` náº¿u khÃ´ng cáº§n context dÃ i

### 2. **Tiáº¿t kiá»‡m tÃ i nguyÃªn**
```bash
# Sá»­ dá»¥ng quantized model
ollama pull gpt-oss:20b-q4_0  # 4-bit quantization
```

### 3. **Monitoring**
```bash
# Monitor GPU
watch nvidia-smi

# Monitor logs
ollama logs
```

---

## ğŸ“ Há»— trá»£

- **Ollama Docs**: https://github.com/ollama/ollama
- **Model Info**: https://huggingface.co/microsoft/gpt-oss-20b
- **Issues**: Táº¡o issue trong repo nÃ y

---

## âœ… Checklist Setup

- [ ] CÃ i Ä‘áº·t Ollama
- [ ] Táº£i model gpt-oss:20b  
- [ ] Test model hoáº¡t Ä‘á»™ng
- [ ] Cáº¥u hÃ¬nh file .env
- [ ] CÃ i Python dependencies
- [ ] Test API connection
- [ ] Cháº¡y Flask app
- [ ] Test web interface
- [ ] Kiá»ƒm tra RAG pipeline

**ğŸ‰ Setup hoÃ n táº¥t! Model Ä‘Ã£ sáºµn sÃ ng phá»¥c vá»¥.**
